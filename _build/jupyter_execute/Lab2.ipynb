{
 "cells": [
  {
   "block_group": "4e0d056591954bea9c0b233ec8bd25f1",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ceecf9b332b14f92b83991acd1997903",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Lab 2 - Math 178, Spring 2024\n",
    "\n",
    "You are encouraged to work in groups of up to 3 total students, but each student should submit their own file. (It's fine for everyone in the group to submit the same link.)\n",
    "\n",
    "Put the full names of everyone in your group (even if you're working alone) here. This makes grading easier.\n",
    "\n",
    "**Names**:"
   ]
  },
  {
   "block_group": "6e9fdb1a86d24be189bb9e52a82ece66",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0a0f5c2ae6074fdbb2c77aa6f2543742",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "The attached data is a very slightly altered form of this [Kaggle dataset](https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/data)."
   ]
  },
  {
   "block_group": "5717d47eed134edc91182ddade40c71d",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b198694f632b4bf9bfc7281ea5f5052c",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Read in the attached credit card fraud data and look at its contents.  Pay particular attention to the column data types.  In this lab, we are interested in predicting the contents of the \"fraud\" column."
   ]
  },
  {
   "block_group": "5d38977f51e744039e587a8d6b4cbaff",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5997cbbe3b94a1595920948ebc768db",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "Divide the data into a training set and a test set.  Specify a `random_state` when you call `train_test_split`, so that you get consistent results.  I had trouble in the logistic regression section if my training set was too big, so I recommend using only 10% of the data (still a lot, 100,000 rows) as the training size.  It's possible that using even a smaller training size is appropriate."
   ]
  },
  {
   "block_group": "36204b1bc2ad4f548222563efa7dd84c",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "38b0e0a2394046f7987b78b7d64d2a59",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Imagine we always predict \"Not Fraud\".  What accuracy score (i.e., proportion correctly classified) do we get on the training set?  On the test set?  Why can there not be any overfitting here?"
   ]
  },
  {
   "block_group": "a2deb207ecfa4ce4b4f7c87722008030",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e2da3e90b3c349bf8cc76421026ace61",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Logistic regression - using scikit-learn\n",
    "\n",
    "Fit a scikit-learn `LogisticRegression` classification model to the training data.  Because it is such a large dataset, I ran into errors/warnings during the `fit` stage if I had instantiated the `LogisticRegression` object using the default parameters.   To combat this, I used only 10% of the data in my training set, I increased the default number of iterations, and I changed the solver.  You can see the options in the `LogisticRegression` class [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).  Originally I also increased the default tolerance, but it seems like this makes the model less accurate, so try to avoid increasing the tolerance if possible.  Don't be surprised if fitting the model takes up to 5 minutes.  If you're having issues, try increasing the tolerance very slightly."
   ]
  },
  {
   "block_group": "95717a4a808e4f728614bc00bdcf6782",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "feebab9bb0fd4c92816dad2a65c4b2c6",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* What is the accuracy score on the training set?  On the test set?  Are you concerned about overfitting?"
   ]
  },
  {
   "block_group": "56b096f291664ab3ac93ec1de79ce3c9",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "73a18ae6c5664fcb96f7d7e137134831",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Evaluate the scikit-learn `confusion_matrix` function on the test data ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)).  Which entry in this confusion matrix would you focus the most on, if you were a bank?  Why? "
   ]
  },
  {
   "block_group": "fbba5c1b1e604abdaf3eed63f5ca6f4a",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ae1021bad4194a11a89b068aab25be1a",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Naive Bayes - by hand\n",
    "\n",
    "Our goal in this section is to perform Naive Bayes \"by hand\" (or at least without using a scikit-learn model).  Recall that Naive Bayes is based on the following formula, taken from Section 4.4 of ISLP:\n",
    "\n",
    "![Formula 4.30](naiveBayes.png)\n",
    "\n",
    "In our case, $k$ will represent either \"Fraud\" or \"Not Fraud\".  The function $f_{ki}(x_i)$ represents the probability (or probability density) of the i-th predictor being $x_i$ in class $k$.  To estimate these functions $f_{ki}$, we will use the first and third bullet points beneath Equation (4.30) in ISLP, according to whether the variable is a float type or a Boolean type.  The term $\\pi_k$ represents *prior* probability of class $k$ (*prior* meaning without dependence on the predictors $x_i$).\n",
    "\n",
    "Strategy:\n",
    "* We first compute the values $\\pi_k$.\n",
    "* We then (prepare to) compute the functions $f_{ki}$ when $i$ represents a float column.\n",
    "* We then (prepare to) compute the functions $f_{ki}$ when $i$ represents a Boolean column."
   ]
  },
  {
   "block_group": "698256e5fe31438eb8ee104ba5d9e00f",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "86e67a649648469487352a874d33b763",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Define a dictionary `prior_dct` representing the two prior values $\\pi_{Fraud}$ and $\\pi_{Not Fraud}$, as in the following template.\n",
    "```\n",
    "prior_dct = {\n",
    "    \"Fraud\": ???,\n",
    "    \"Not Fraud\": ???\n",
    "}\n",
    "```\n",
    "Reality check: the two values should sum to (approximately) 1."
   ]
  },
  {
   "block_group": "85315696c2c64819b406213e02840fa9",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "83083f77e977495abb8756d75e1de272",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* It's temporarily convenient here to have `X_train` and `y_train` together in the same DataFrame.  Concatenate these together along the columns axis and name the result `df_train`."
   ]
  },
  {
   "block_group": "4b1ca6e1c57f4ec4a2bf754fed679752",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0f3eb15c1d8b4df1aaadc46c09cc6062",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Write a function `Gaussian_helper` which takes a DataFrame input `df` and two string inputs, a class `k` (which will be \"Not Fraud\" or \"Fraud\" in our case) and a column name `col` of one of the float columns.  The output should be a dictionary with keys `\"mean\"` and `\"std\"`, representing the mean and the standard deviation for the given column within the given class, as in the first bullet point after (4.30).  \n",
    "\n",
    "Comment: To find the mean and standard deviation, you can use the formulas in (4.20) (take the square root of the variance to get the standard deviation), but I think it's easier to just let pandas compute these for you, using the `mean` and `std` methods of a pandas Series.  It's possible pandas will use $n$ instead of the $n-K$ in Equation (4.20), but that shouldn't be significant here because $n$ is so big and $K=2$. \n",
    "\n",
    "Here is a possible template:\n",
    "```\n",
    "def Gaussian_helper(df, k, col):\n",
    "    output_dct = {}\n",
    "    ... # one or more lines here\n",
    "    output_dct[\"mean\"] = ...\n",
    "    output_dct[\"std\"] = ...\n",
    "    return output_dct\n",
    "```"
   ]
  },
  {
   "block_group": "ecf9167355b8418eb8eb5b4df8ff4c72",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9da9f263800f42fb8fdb61db5a6dfc5a",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Similarly, write a function `Boolean_helper` which takes a DataFrame input `df` and two string inputs, a class `k` (which will be \"Not Fraud\" or \"Fraud\" in our case) and a column name `col` of one of the Boolean columns.  The output should be a dictionary with keys `True` and `False`, representing the proportion of these values within the given class.  For an example, see the third bullet point after (4.30) in the textbook.\n",
    "\n",
    "Comment: Make sure your keys are `bool` values, not strings."
   ]
  },
  {
   "block_group": "3ca29c96a936495ab2e863e87835376e",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "98601a7108514e30a83bd36c5cc8fedf",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Check your helper functions by comparing a few of their outputs to the following.  (I feel like there is probably a nice way to use the following DataFrame directly and never define the helper functions, but I did not succeed in doing that.)\n",
    "\n",
    "```\n",
    "df_train.groupby(\"fraud\").mean()\n",
    "```"
   ]
  },
  {
   "block_group": "b7220402d4e943cc91d61340d1be79de",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9a5744f4bc6f49d2a43b37818fa9c421",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Here is an example of applying the Gaussian probability density function with mean 10 and standard deviation 4 to every entry in a column, without explicitly using any loops.\n",
    "\n",
    "```\n",
    "from scipy.stats import norm\n",
    "\n",
    "norm.pdf(X_train[\"distance_from_last_transaction\"], loc=10, scale=4)\n",
    "```\n",
    "\n",
    "You should view the outputs as representing the probability (densities) of the given Gaussian distribution producing the input values."
   ]
  },
  {
   "block_group": "ece93fa5c92c4e0e82b6f1bb10e2089e",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9e3163f0f0094e8aa634b377db583c42",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Here is an example of using a dictionary to replace every value in a column.  Think of the values in this dictionary as our estimated probabilities.\n",
    "\n",
    "```\n",
    "temp_dct = {True: 0.71, False: 0.29}\n",
    "\n",
    "X_train[\"used_chip\"].map(temp_dct)\n",
    "```"
   ]
  },
  {
   "block_group": "9bfafd0c01764c4da0b11ad7b4cb14fa",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "347572bd51904072b85341937efbb9d4",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Momentarily fix the class $k$ to be \"Fraud\".  We are going to compute the numerator of Equation (4.30) for every row of `X_train`.  (Here we switch back to using `X_train` rather than `df_train`.)\n",
    "\n",
    "Do all of the following in a single code cell.  (The reason for not separating the cells is so that the entire cell can be run again easily.)\n",
    "\n",
    "* Assign `k = \"Fraud\"`.\n",
    "* Copy the `X_train` DataFrame into a new DataFrame called `X_temp`.  Use the `copy` method.\n",
    "* For each column of `X_temp`, use `Gaussian_helper` or `Boolean_helper`, as appropriate, to replace each value $x_i$ with $f(x_i)$, where $f$ is as in (4.30).  You can use a for loop to loop over the columns, but within a fixed column, you should not need to use a for loop (in other words, you should not need to loop over the rows, only over the columns).  The following imports might be helpful for determining the data types (make the imports outside of any for loop).\n",
    "```\n",
    "from pandas.api.types import is_bool_dtype, is_float_dtype\n",
    "```\n",
    "\n",
    "Comment: Your code should be changing the `X_temp` entries but not the `X_train` entries.  When you are finished, `X_temp` will be a DataFrame containing probabilities, all corresponding to the \"Fraud\" class.\n",
    "\n",
    "* For each row, multiply all entries in that row.  (Hint.  DataFrames have a `prod` method.)  Also multiply by the prior probability of \"Fraud\".  (Use `k`, do not type `\"Fraud\"`.)  The end result should be a pandas Series corresponding to the numerator of (4.30), for each row of `X_train`.  Don't be surprised if the numbers are very small, like around $10^{-10}$."
   ]
  },
  {
   "block_group": "87d22b842c514ffea95668c2c85cfa85",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b8f455cf0fbb41ef85a2a2b3f5175200",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Once the code is working, wrap the whole thing into another for loop, corresponding to `k = \"Fraud\"` and `k = \"Not Fraud\"`, putting the two resulting pandas Series into a length 2 dictionary with keys `\"Fraud\"` and `\"Not Fraud\"`.  Call this dictionary `num_dct`, because it represents the numerators of (4.30)."
   ]
  },
  {
   "block_group": "9b2a0089105f49b594239c8f35758b63",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "932f645e6a61494f865362737ef4a153",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* Create a new two-column pandas DataFrame with the results using the following code:\n",
    "```\n",
    "df_num = pd.DataFrame(num_dct)\n",
    "```"
   ]
  },
  {
   "block_group": "dbdf7dbe2b7f4e83876ed66ea353a5df",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d0a92717b7734af2bbb1049bf433c6d0",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "* What proportion of the values in `X_train` are correctly identified as Fraud using this procedure?  (Note.  We never actually need to compute the denominator in (4.30), since all we care about here is which entry is bigger.)"
   ]
  },
  {
   "block_group": "8355e878f0244309b7d7690a7a5c0764",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7d5300ba56c64ed5b5655d9f74b1fe9d",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Submission\n",
    "\n",
    "* Using the `Share` button at the top right, enable public sharing, and enable Comment privileges. Then submit the created link on Canvas."
   ]
  },
  {
   "block_group": "7d5fe7c432954a9a8075c0b04bbb47a4",
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5e9f8306d6ec4fa08baf79dd6ed5e426",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Possible extensions\n",
    "\n",
    "* I originally wanted us to consider log loss as our error metric, but I decided the lab was already getting rather long, so I removed that.  But in general, log loss is a more refined measure for detecting overfitting than accuracy score.  It should be relatively straightforward to evaluate log loss for the Logistic Regression model.  Compare this to the log loss of a baseline prediction, where we predict the same probability for every row.  I got some errors when I tried to evaluate log loss for the Naive Bayes model and I haven't thought carefully about how to avoid these.\n",
    "* How do our values compare to using the scikit-learn Naive Bayes model?  (I don't think this will be easy, because you will have to treat the Gaussian and the Boolean portions separately.  There might also be some discrepancy due to our method of estimating standard deviation, but I don't think that is crucial.  I have not tried this myself, so there could also be other discrepancies I'm not anticipating.)\n",
    "* How does KNN compare in performance?  (What's the optimal number of neighbors?)  I haven't tried this, and I think the training size might be too large, so be prepared to reduce the size of the training set further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=f2ec4f7b-862a-4f2a-b82a-fc00a94f2f21' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "75fbce230c4147a7bb8018ffdfa49bd5",
  "deepnote_persisted_session": {
   "createdAt": "2024-04-15T17:16:09.760Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}